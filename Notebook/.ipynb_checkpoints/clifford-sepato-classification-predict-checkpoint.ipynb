{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Change Belief Analysis Using Twitter Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![climate_change_activists]( https://raw.githubusercontent.com/cliffordsepato/climate_change_classification/main/images/youth-climate-activists.jpg)\n",
    "<p align=\"center\">Ollie Millington / Getty Images </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-one\"></a>\n",
    "# Introduction\n",
    "\n",
    "Climate change is a global phenomenon that involves ongoing changes in average temperatures and weather patterns.\n",
    "Extreme weather events and catastrophes emanating from global warming such as hurricanes, floods and fires, have a direct impact on 70% of all economic sectors worldwide.\n",
    "\n",
    "Many business leaders are now acknowledging the business imperative of climate change and increasingly understand it to be an existential threat that has far-reaching consequences for their people and business operations. An increasily warmer planet creates a wide range of risks for businesses, from disrupted supply chains to rising insurance costs and labor challenges.\n",
    "\n",
    "This new reality has forced many companies to explore ways to lessening their environmental impact and carbon footprint through offering products and services that are environmentally friendly, sustainable, and in line with their values and ideals. \n",
    "\n",
    "Social media is a powerful source of information on a wide range of topic. Data from popular social media platforms such as twitter can be harvested and analyzed to find trends related to specific topics,measure poplular sentiment,obtain feedback on past decison and also help shape future decision.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-two\"></a>\n",
    "# Problem Statement\n",
    "The aim of this project is to gauge the public perception of climate change using twitter data. This will help companies to: \n",
    "* Access broad base consumer sentiments, and insights in order to inform future marketing strategies.\n",
    "* Gain insights on people's views and perceptions about climate change. \n",
    "* Improve market research efforts for companies that provide environmentally-friendly products and services.\n",
    "* Strengthen efforts to reduce carbon footprint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-three\"></a>\n",
    "# Objective\n",
    "* Build a Machine Learning model that is able to classify whether or not aperson believes in climate change, based on their novel tweet data\n",
    "* Provide a accurate robust solution that gives companies access to a broad base of customer sentiments,thus increasing their insights and informing future marketing strategies. \n",
    "* Achieve a Mean F1-score higher than 0.70.\n",
    "*  Build an App using Streamlit's open-source app framework.\n",
    "*  Host app within an AWS EC2 instance.\n",
    "*  Use Comet alternate version control methods for experiments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-four\"></a>\n",
    "# Import libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-10-23T19:24:07.316576Z",
     "iopub.status.busy": "2022-10-23T19:24:07.315563Z",
     "iopub.status.idle": "2022-10-23T19:25:19.913581Z",
     "shell.execute_reply": "2022-10-23T19:25:19.912566Z",
     "shell.execute_reply.started": "2022-10-23T19:24:07.316475Z"
    }
   },
   "outputs": [],
   "source": [
    "# install libraries\n",
    "!pip install comet_ml\n",
    "!pip install spacy\n",
    "!pip install seaborn\n",
    "!pip install wordcloud\n",
    "!pip install emoji\n",
    "!pip install pyspellchecker\n",
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-10-23T19:30:21.698803Z",
     "iopub.status.busy": "2022-10-23T19:30:21.697922Z",
     "iopub.status.idle": "2022-10-23T19:30:22.829737Z",
     "shell.execute_reply": "2022-10-23T19:30:22.828856Z",
     "shell.execute_reply.started": "2022-10-23T19:30:21.698771Z"
    }
   },
   "outputs": [],
   "source": [
    "# download NLP package\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:30:33.123636Z",
     "iopub.status.busy": "2022-10-23T19:30:33.123278Z",
     "iopub.status.idle": "2022-10-23T19:30:33.440408Z",
     "shell.execute_reply": "2022-10-23T19:30:33.439508Z",
     "shell.execute_reply.started": "2022-10-23T19:30:33.123608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Packages  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# Packages for visualisations\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "sns.set(font_scale=1.5)\n",
    "import matplotlib.style as style\n",
    "style.use('seaborn-pastel')\n",
    "style.use('seaborn-poster')\n",
    "from PIL import Image\n",
    "\n",
    "# Packages for preprocessing\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.probability import FreqDist\n",
    "import emoji\n",
    "from ftfy import fix_text\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Packages for training models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Packages for hyperparameter optimisation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Packages for evaluating model accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Packages for saving models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"subsection-one\"></a>\n",
    "# Start Experiment in Comet\n",
    "Using Comet alternate version control methods for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:30:38.605962Z",
     "iopub.status.busy": "2022-10-23T19:30:38.604883Z",
     "iopub.status.idle": "2022-10-23T19:30:39.997152Z",
     "shell.execute_reply": "2022-10-23T19:30:39.996254Z",
     "shell.execute_reply.started": "2022-10-23T19:30:38.605910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import package for creating an experiment in Comet\n",
    "import comet_ml\n",
    "from comet_ml import Experiment\n",
    "# Setting the API key (saved as environment variable)\n",
    "experiment = Experiment(api_key='Wg4tgIhuwsRj6kHdY3CCK6rmD',\n",
    "                        project_name=\"climate_change_classification_predict\", workspace=\"cliffordsepato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"subsection-two\"></a>\n",
    "# Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:30:43.780317Z",
     "iopub.status.busy": "2022-10-23T19:30:43.779960Z",
     "iopub.status.idle": "2022-10-23T19:30:43.913644Z",
     "shell.execute_reply": "2022-10-23T19:30:43.912964Z",
     "shell.execute_reply.started": "2022-10-23T19:30:43.780284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import training dataset\n",
    "df_train = pd.read_csv('../input/edsa-sentiment-classification/train.csv')\n",
    "\n",
    "# Import testing dataset\n",
    "df_test = pd.read_csv('../input/edsa-sentiment-classification/test.csv')\n",
    "\n",
    "# Set 'tweetid' index\n",
    "df_train.set_index('tweetid',inplace = True)\n",
    "df_test.set_index('tweetid',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"subsection-three\"></a>\n",
    "# Reading the Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:30:46.614907Z",
     "iopub.status.busy": "2022-10-23T19:30:46.614583Z",
     "iopub.status.idle": "2022-10-23T19:30:46.634529Z",
     "shell.execute_reply": "2022-10-23T19:30:46.633433Z",
     "shell.execute_reply.started": "2022-10-23T19:30:46.614881Z"
    }
   },
   "outputs": [],
   "source": [
    "# A look at the data structure\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "display (df_train.head())\n",
    "display (df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-six\"></a>\n",
    "# About the Data\n",
    "* The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. \n",
    "* The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018 \n",
    "* In total, <b>43,943</b> tweets were collected. \n",
    "* Each tweet is labelled as one of 4 classes.\n",
    "<h3> Class Description:</h3>\n",
    "\n",
    "* <b>Class 2 News:</b> the tweet links to factual news about climate change\n",
    "* <b>Class1 Pro:</b> the tweet supports the belief of man-made climate change\n",
    "* <b>Class 0 Neutral:</b> the tweet neither supports nor refutes the belief of man-made climate change.\n",
    "* <b>Class-1  Anti: </b> the tweet does not believe in man-made climate change Variable definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-seven\"></a>\n",
    "# Preprocessing\n",
    "* Extract tweet sentiments\n",
    "* Extract hashtags\n",
    "* Replace contractions\n",
    "* Remove URLS, emojis,punctuations,mentions, numbers and extra white space\n",
    "* Convert all text to lowercase\n",
    "* Replace shortened words/slang\n",
    "* Tokenization\n",
    "* Perform lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:30:51.562237Z",
     "iopub.status.busy": "2022-10-23T19:30:51.561793Z",
     "iopub.status.idle": "2022-10-23T19:30:51.586203Z",
     "shell.execute_reply": "2022-10-23T19:30:51.585308Z",
     "shell.execute_reply.started": "2022-10-23T19:30:51.562197Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a copy of the train dataframe\n",
    "def update(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates a copy of the original train data and \n",
    "    renames the classes, converting them from numbers to words\n",
    "    \n",
    "    Input: \n",
    "    df: original dataframe\n",
    "        datatype: dataframe\n",
    "    \n",
    "    Output:\n",
    "    df: modified dataframe\n",
    "        datatype: dataframe \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    df = df_train.copy()\n",
    "    sentiment = df['sentiment']\n",
    "    word_sentiment = []\n",
    "\n",
    "    for i in sentiment :\n",
    "        if i == 1 :\n",
    "            word_sentiment.append('Pro')\n",
    "        elif i == 0 :\n",
    "            word_sentiment.append('Neutral')\n",
    "        elif i == -1 :\n",
    "            word_sentiment.append('Anti')\n",
    "        else :\n",
    "            word_sentiment.append('News')\n",
    "\n",
    "    df['sentiment'] = word_sentiment\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = update(df_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Hashtags\n",
    "Extracting hashtags for original tweets and storing them in seperate dataframes for each class before first cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:30:57.001938Z",
     "iopub.status.busy": "2022-10-23T19:30:57.001588Z",
     "iopub.status.idle": "2022-10-23T19:30:57.139628Z",
     "shell.execute_reply": "2022-10-23T19:30:57.138666Z",
     "shell.execute_reply.started": "2022-10-23T19:30:57.001909Z"
    }
   },
   "outputs": [],
   "source": [
    "def hashtag_extract(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in a tweet and extracts the top 15 hashtag(s) using regular expressions\n",
    "    These hashtags are stored in a seperate dataframe \n",
    "    along with a count of how frequenty they occur\n",
    "    \n",
    "    Input:\n",
    "    tweet: original tweets\n",
    "           datatype: 'str'\n",
    "           \n",
    "    Output:\n",
    "    hashtag_df: dataframe containing the top hashtags in the tweets\n",
    "              datatype: dataframe         \n",
    "    \"\"\"\n",
    "    \n",
    "    hashtags = []\n",
    "    \n",
    "    for i in tweet:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "        \n",
    "    hashtags = sum(hashtags, [])\n",
    "    frequency = nltk.FreqDist(hashtags)\n",
    "    \n",
    "    hashtag_df = pd.DataFrame({'hashtag': list(frequency.keys()),\n",
    "                       'count': list(frequency.values())})\n",
    "    hashtag_df = hashtag_df.nlargest(15, columns=\"count\")\n",
    "\n",
    "    return hashtag_df\n",
    "\n",
    "# Extracting the hashtags from tweets in each class\n",
    "pro = hashtag_extract(df['message'][df['sentiment'] == 'Pro'])\n",
    "anti = hashtag_extract(df['message'][df['sentiment'] == 'Anti'])\n",
    "neutral = hashtag_extract(df['message'][df['sentiment'] == 'Neutral'])\n",
    "news = hashtag_extract(df['message'][df['sentiment'] == 'News'])\n",
    "\n",
    "pro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Sentiments of Tweets\n",
    "classifying all tweets with scores of less than -0.05 as negative, scores between -0.05 and 0.05 are classified as neutral and a score of more than 0.05 indicates a positive tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:30:59.560101Z",
     "iopub.status.busy": "2022-10-23T19:30:59.559730Z",
     "iopub.status.idle": "2022-10-23T19:30:59.566124Z",
     "shell.execute_reply": "2022-10-23T19:30:59.565219Z",
     "shell.execute_reply.started": "2022-10-23T19:30:59.560072Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract sentiment\n",
    "def sentiment_score(text):\n",
    "    \"\"\" A function that determines the sentiment of a text string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text: Text string.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sentiment:  String indicating the sentiment of the input string.\n",
    "    \"\"\"\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    s = sid.polarity_scores(text)['compound']\n",
    "    if s<-0.05:\n",
    "        sentiment='negative'\n",
    "    elif s>0.05:\n",
    "        sentiment='positive'\n",
    "    else:\n",
    "        sentiment='neutral'\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:04.906114Z",
     "iopub.status.busy": "2022-10-23T19:31:04.905692Z",
     "iopub.status.idle": "2022-10-23T19:31:04.963676Z",
     "shell.execute_reply": "2022-10-23T19:31:04.963047Z",
     "shell.execute_reply.started": "2022-10-23T19:31:04.906080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract all unique news related handles into a list\n",
    "df_temp = df_train.copy()\n",
    "df_temp.sort_index(inplace=True)\n",
    "n_temp = [re.findall(r'@[\\w]+',df_temp['message'].iloc[i]) for i,x in enumerate(df_temp['sentiment']) if x==2]\n",
    "news = [x for x in n_temp if x!=[]]\n",
    "\n",
    "# Only keep the unique values inside the list\n",
    "news = sorted(list(set(itertools.chain.from_iterable(news))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:08.266799Z",
     "iopub.status.busy": "2022-10-23T19:31:08.266495Z",
     "iopub.status.idle": "2022-10-23T19:31:08.289957Z",
     "shell.execute_reply": "2022-10-23T19:31:08.289070Z",
     "shell.execute_reply.started": "2022-10-23T19:31:08.266774Z"
    }
   },
   "outputs": [],
   "source": [
    "# extracting all unque hashtags\n",
    "hashtags = df['message'].apply(lambda x: re.findall(r'[#]\\\\w+',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:10.623706Z",
     "iopub.status.busy": "2022-10-23T19:31:10.623395Z",
     "iopub.status.idle": "2022-10-23T19:31:10.631738Z",
     "shell.execute_reply": "2022-10-23T19:31:10.630603Z",
     "shell.execute_reply.started": "2022-10-23T19:31:10.623683Z"
    }
   },
   "outputs": [],
   "source": [
    "hashtags = list(set([item for sublist in hashtags for item in sublist]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next steps, we will be removing URL's, punctuations, mentions, numbers, extra white spaces and normalizing by converting all letters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:15.525728Z",
     "iopub.status.busy": "2022-10-23T19:31:15.525331Z",
     "iopub.status.idle": "2022-10-23T19:31:15.533320Z",
     "shell.execute_reply": "2022-10-23T19:31:15.532704Z",
     "shell.execute_reply.started": "2022-10-23T19:31:15.525694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary of contracted words\n",
    "contractions = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we'll\":\"we will\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:19.648408Z",
     "iopub.status.busy": "2022-10-23T19:31:19.648015Z",
     "iopub.status.idle": "2022-10-23T19:31:19.764920Z",
     "shell.execute_reply": "2022-10-23T19:31:19.764042Z",
     "shell.execute_reply.started": "2022-10-23T19:31:19.648377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace contracted words with full words\n",
    "df_train['message'] = [' '.join([contractions[w.lower()] if w.lower() in contractions.keys() else w for w in raw.split()]) \n",
    "                       for raw in df_train['message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:30.265127Z",
     "iopub.status.busy": "2022-10-23T19:31:30.264304Z",
     "iopub.status.idle": "2022-10-23T19:31:30.281284Z",
     "shell.execute_reply": "2022-10-23T19:31:30.280382Z",
     "shell.execute_reply.started": "2022-10-23T19:31:30.265082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lower case all words to remove noise from Capital words. Capital words may be seen as different from lower case words\n",
    "df_train['message'] = df_train['message'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:32.951314Z",
     "iopub.status.busy": "2022-10-23T19:31:32.950935Z",
     "iopub.status.idle": "2022-10-23T19:31:34.846735Z",
     "shell.execute_reply": "2022-10-23T19:31:34.845843Z",
     "shell.execute_reply.started": "2022-10-23T19:31:32.951288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fix all the bad unicode to allow better processing of the data\n",
    "df_train['message'] = df_train['message'].apply(lambda x: fix_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:37.709810Z",
     "iopub.status.busy": "2022-10-23T19:31:37.709484Z",
     "iopub.status.idle": "2022-10-23T19:31:37.760835Z",
     "shell.execute_reply": "2022-10-23T19:31:37.759911Z",
     "shell.execute_reply.started": "2022-10-23T19:31:37.709781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing urls\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'https\\S+','url',x))\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'www\\S+', 'url',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:40.091921Z",
     "iopub.status.busy": "2022-10-23T19:31:40.090935Z",
     "iopub.status.idle": "2022-10-23T19:31:40.644680Z",
     "shell.execute_reply": "2022-10-23T19:31:40.643653Z",
     "shell.execute_reply.started": "2022-10-23T19:31:40.091887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace emojis with their word meaning\n",
    "df_train['message'] = df_train['message'].apply(lambda x: emoji.demojize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:31:44.604352Z",
     "iopub.status.busy": "2022-10-23T19:31:44.603978Z",
     "iopub.status.idle": "2022-10-23T19:31:44.619464Z",
     "shell.execute_reply": "2022-10-23T19:31:44.618504Z",
     "shell.execute_reply.started": "2022-10-23T19:31:44.604324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace shortened words with full words\n",
    "short = {' BD ': ' Big Deal ',\n",
    " ' abt ':' about ',\n",
    " ' ab ': ' about ',\n",
    " ' fav ': ' favourite ',\n",
    " ' fab ': ' fabulous ',\n",
    " ' smh ': ' shaking my head ',\n",
    " ' u ': ' you ',\n",
    " ' c ': ' see ',\n",
    " ' anon ': ' anonymous ',\n",
    " ' ac ': ' aircon ',\n",
    " ' a/c ': ' aircon ',\n",
    " ' yo ':' year old ',\n",
    " ' n ':' and ',\n",
    " ' nd ':' and ',\n",
    " ' 2 ': ' to ',\n",
    " ' w ': ' with ',\n",
    " ' w/o ': ' without ',\n",
    " ' r ': ' are ',\n",
    " ' rip ':' rest in peace ',\n",
    " ' 4 ' : ' for ',\n",
    "' BF ': ' Boyfriend ',\n",
    "' BRB ': ' Be Right Back ',\n",
    "' BTW ': ' By The Way ',\n",
    "' GF ': ' Girlfriend ',\n",
    "' HBD ': ' Happy Birthday ',\n",
    "' JK ': ' Just Kidding ',\n",
    "' K ':' Okay ',\n",
    "' LMK ': ' Let Me Know ',\n",
    "' LOL ': ' Laugh Out Loud ',\n",
    "' HA ':' laugh ',\n",
    "' MYOB ': ' Mind Your Own Business ',\n",
    "' NBD ': ' No Big Deal ',\n",
    "' NVM ': ' Nevermind ',\n",
    "' Obv ':' Obviously ',\n",
    "' Obvi ':' Obviously ',\n",
    "' OMG ': ' Oh My God ',\n",
    "' Pls ': ' Please ',\n",
    "' Plz ': ' Please ',\n",
    "' Q ': ' Question ', \n",
    "' QQ ': ' Quick Question ',\n",
    "' RLY ': ' Really ',\n",
    "' SRLSY ': ' Seriously ',\n",
    "' TMI ': ' Too Much Information ',\n",
    "' TY ': ' Thank You, ',\n",
    "' TYVM ': ' Thank You Very Much ',\n",
    "' YW ': ' You are Welcome ',\n",
    "' FOMO ': ' Fear Of Missing Out ',\n",
    "' FTFY ': ' Fixed This For You ',\n",
    "' FTW ': ' For The Win ',\n",
    "' FYA ': ' For Your Amusement ',\n",
    "' FYE ': ' For Your Entertainment ',\n",
    "' GTI ': ' Going Through It ',\n",
    "' HTH ': ' Here to Help ',\n",
    "' IRL ': ' In Real Life ',\n",
    "' ICYMI ': ' In Case You Missed It ',\n",
    "' ICYWW ': ' In Case You Were Wondering ',\n",
    "' NBC ': ' Nobody Cares Though ',\n",
    "' NTW ': ' Not To Worry ',\n",
    "' OTD ': ' Of The Day ',\n",
    "' OOTD ': ' Outfit Of The Day ',\n",
    "' QOTD ': ' Quote of the Day ',\n",
    "' FOTD ': ' Find Of the Day ',\n",
    "' POIDH ': ' Pictures Or It Did ntt Happen ',\n",
    "' YOLO ': ' You Only Live Once ',\n",
    "' AFAIK ': ' As Far As I Know ',\n",
    "' DGYF ': ' Dang Girl You Fine ',\n",
    "' FWIW ': ' For What It is Worth ',\n",
    "' IDC ': ' I Do not Care ',\n",
    "' IDK ': ' I Do not Know ',\n",
    "' IIRC ': ' If I Remember Correctly ',\n",
    "' IMHO ': ' In My Honest Opinion ',\n",
    "' IMO ': ' In My Opinion ',\n",
    "' Jelly ': ' Jealous ',\n",
    "' Jellz ': ' Jealous ',\n",
    "' JSYK ': ' Just So You Know ',\n",
    "' LMAO ': ' Laughing My Ass Off ',\n",
    "' LMFAO ': ' Laughing My Fucking Ass Off ',\n",
    "' NTS ': ' Note to Self ',\n",
    "' ROFL ': ' Rolling On the Floor Laughing ',\n",
    "' ROFLMAO ': ' Rolling On the Floor Laughing My Ass Off ',\n",
    "' SMH ': ' Shaking My Head ',\n",
    "' TBH ': ' To Be Honest ',\n",
    "' TL;DR ':  ' Too Long; Did not Read ',\n",
    "' TLDR ':  ' Too Long; Did not Read ',\n",
    "' YGTR ': ' You Got That Right ',\n",
    "' AYKMWTS ': ' Are You Kidding Me With This Shit ',\n",
    "' BAMF ': ' Bad Ass Mother Fucker ',\n",
    "' FFS ': ' For Fuck Sake ',\n",
    "' FML ': ' Fuck My Life ',\n",
    "' HYFR ': ' Hell Yeah Fucking Right ',\n",
    "' IDGAF ': ' I Do not Give A Fuck ',\n",
    "' NFW ': ' No Fucking Way ',\n",
    "' PITA ': ' Pain In The Ass ',\n",
    "' POS ': ' Piece of Shit ',\n",
    "' SOL ': ' Shit Outta Luck ',\n",
    "' STFU ': ' Shut the Fuck Up ',\n",
    "' TF ': ' The Fuck ',\n",
    "' WTF ': ' What The Fuck ',\n",
    "' BFN ': ' Bye For Now ',\n",
    "' CU ': ' See You ',\n",
    "' IC ': ' I see ',\n",
    "' CYL ': ' See You Later ',\n",
    "' GTG ': ' Got to Go ',\n",
    "' OMW ': ' On My Way ',\n",
    "' RN ': ' Right Now ',\n",
    "' TTYL ': ' Talk To You Later ',\n",
    "' TYT ': ' Take Your time ',\n",
    "' CC ': ' Carbon Copy ',\n",
    "' CX ': ' Correction ',\n",
    "' DM ': ' Direct Message ',\n",
    "' FB ': ' Facebook ',\n",
    "' FBF ': ' Flash-Back Friday ',\n",
    "' FF ': ' Follow Friday ',\n",
    "' HT ': ' Tipping my hat ',\n",
    "' H/T ': ' Tipping my hat ',\n",
    "' IG ': ' Instagram ',\n",
    "' Insta ': ' Instagram ',\n",
    "' MT ':' Modified Tweet ',\n",
    "' OH ': ' Overheard ',\n",
    "' PRT ': ' Partial Retweet ',\n",
    "' RT ': ' Retweet ',\n",
    "'rt ' : ' retweet ',\n",
    "' SO ':' Shout Out ',\n",
    "' S/O ': ' Shout Out ',\n",
    "' TBT ': ' Throw-Back Thursday ',\n",
    "' AWOL ': ' Away While Online ',\n",
    "' BFF ': ' Best Friend Forever ',\n",
    "' NSFW ': ' Not Safe For Work ',\n",
    "' OG ': ' Original Gangster ',\n",
    "' PSA ': ' Public Service Announcement ',\n",
    "' PDA ': ' Public Display of Affection '}\n",
    "\n",
    "short = dict((key.lower(), value.lower()) for key,value in short.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:32:08.492944Z",
     "iopub.status.busy": "2022-10-23T19:32:08.492623Z",
     "iopub.status.idle": "2022-10-23T19:32:11.598789Z",
     "shell.execute_reply": "2022-10-23T19:32:11.597746Z",
     "shell.execute_reply.started": "2022-10-23T19:32:08.492920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing shortened words with full alternative words\n",
    "for word in short.keys():\n",
    "    df_train['message'] = df_train['message'].apply(lambda x: re.sub(word,short[word],x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:32:14.117552Z",
     "iopub.status.busy": "2022-10-23T19:32:14.117070Z",
     "iopub.status.idle": "2022-10-23T19:32:20.109384Z",
     "shell.execute_reply": "2022-10-23T19:32:20.108433Z",
     "shell.execute_reply.started": "2022-10-23T19:32:14.117515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove twitter non news related handles and @ symbol\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'@', '', ' '.join([y for y in x.split() if y not in \n",
    "                                                                                     [z for z in re.findall(r'@[\\w]*',x) \n",
    "                                                                                      if z not in news]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:32:26.580375Z",
     "iopub.status.busy": "2022-10-23T19:32:26.580025Z",
     "iopub.status.idle": "2022-10-23T19:34:07.171360Z",
     "shell.execute_reply": "2022-10-23T19:34:07.170421Z",
     "shell.execute_reply.started": "2022-10-23T19:32:26.580350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add sentiment\n",
    "df_train['message'] = df_train['message'].apply(lambda x: x + ' ' + sentiment_score(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:35:04.450657Z",
     "iopub.status.busy": "2022-10-23T19:35:04.450246Z",
     "iopub.status.idle": "2022-10-23T19:35:04.723191Z",
     "shell.execute_reply": "2022-10-23T19:35:04.722297Z",
     "shell.execute_reply.started": "2022-10-23T19:35:04.450628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r\"[^A-Za-z ]*\",'',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:35:07.764804Z",
     "iopub.status.busy": "2022-10-23T19:35:07.764492Z",
     "iopub.status.idle": "2022-10-23T19:35:07.877541Z",
     "shell.execute_reply": "2022-10-23T19:35:07.876490Z",
     "shell.execute_reply.started": "2022-10-23T19:35:07.764780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove repeated vowels \n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r'([aeiou])\\1+', r'\\1\\1', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:35:11.273634Z",
     "iopub.status.busy": "2022-10-23T19:35:11.273232Z",
     "iopub.status.idle": "2022-10-23T19:35:11.369721Z",
     "shell.execute_reply": "2022-10-23T19:35:11.368817Z",
     "shell.execute_reply.started": "2022-10-23T19:35:11.273600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace slang words\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r' ha([ha]) *', r'laugh', x))\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r' he([he]) *', r'laugh', x))\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r' lol([ol]) *', r'laugh', x))\n",
    "df_train['message'] = df_train['message'].apply(lambda x: re.sub(r' lo([o])*l ', r'laugh', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "We'll write a function to apply all the transformations that were applied above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:35:15.911322Z",
     "iopub.status.busy": "2022-10-23T19:35:15.910403Z",
     "iopub.status.idle": "2022-10-23T19:35:15.920347Z",
     "shell.execute_reply": "2022-10-23T19:35:15.919522Z",
     "shell.execute_reply.started": "2022-10-23T19:35:15.911283Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(raw):\n",
    "    \"\"\" A function that 'cleans' tweet data. The text gets modified by:\n",
    "        - being lower cased, \n",
    "        - removing urls, \n",
    "        - removing bad unicode,\n",
    "        - replacing emojis with words,\n",
    "        - removing twitter non news related handles,\n",
    "        - removing punctuation,\n",
    "        - removing vowels repeated at least 3 times,\n",
    "        - replacing sequences of 'h' and 'a', as well as 'lol' with 'laugh',\n",
    "        - adding sentiment\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw: Text string.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        raw:  Modified clean string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    raw = raw.lower()\n",
    "    \n",
    "    # Fix strange characters\n",
    "    raw = fix_text(raw)\n",
    "    \n",
    "    # Removing urls\n",
    "    raw = re.sub(r'https\\S+','url',raw)\n",
    "    raw = re.sub(r'www\\S+', 'url',raw)\n",
    "    \n",
    "    # Replace emojis with their word meaning\n",
    "    raw = emoji.demojize(raw)\n",
    "\n",
    "    # Remove twitter non news related handles\n",
    "    raw = ' '.join([y for y in raw.split() if y not in [x for x in re.findall(r'@[\\w]*',raw) if x not in news]])\n",
    "    \n",
    "    # Add sentiment\n",
    "    raw = raw + ' ' + sentiment_score(raw)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    raw = re.sub(r\"[^A-Za-z ]*\",'',raw)\n",
    "    \n",
    "    # Remove repeated vowels \n",
    "    raw = re.sub(r'([aeiou])\\1+', r'\\1\\1', raw)\n",
    "    \n",
    "    # Replace slang words'\n",
    "    raw = re.sub(r' ha([ha]) *', r'laugh', raw)\n",
    "    raw = re.sub(r' he([he]) *', r'laugh', raw)\n",
    "    raw = re.sub(r' lol([ol]) *', r'laugh', raw)\n",
    "    raw = re.sub(r' lo([o])*l ', r'laugh', raw)\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:35:25.297636Z",
     "iopub.status.busy": "2022-10-23T19:35:25.297289Z",
     "iopub.status.idle": "2022-10-23T19:35:25.389530Z",
     "shell.execute_reply": "2022-10-23T19:35:25.388495Z",
     "shell.execute_reply.started": "2022-10-23T19:35:25.297608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace contracted words with full word\n",
    "df_test['message'] = [' '.join([contractions[w.lower()] if w.lower() in contractions.keys() else w for w in raw.split()]) \n",
    "                      for raw in df_test['message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:35:26.883673Z",
     "iopub.status.busy": "2022-10-23T19:35:26.883368Z",
     "iopub.status.idle": "2022-10-23T19:35:28.850592Z",
     "shell.execute_reply": "2022-10-23T19:35:28.849536Z",
     "shell.execute_reply.started": "2022-10-23T19:35:26.883649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing shortened words with full words\n",
    "for word in short.keys():\n",
    "    df_test['message'] = df_test['message'].apply(lambda x: re.sub(word,short[word],x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:39:28.841989Z",
     "iopub.status.busy": "2022-10-23T19:39:28.841384Z",
     "iopub.status.idle": "2022-10-23T19:40:37.963698Z",
     "shell.execute_reply": "2022-10-23T19:40:37.962792Z",
     "shell.execute_reply.started": "2022-10-23T19:39:28.841962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply cleaning function\n",
    "df_test['message'] = df_test['message'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:41:15.134604Z",
     "iopub.status.busy": "2022-10-23T19:41:15.134352Z",
     "iopub.status.idle": "2022-10-23T19:41:18.765869Z",
     "shell.execute_reply": "2022-10-23T19:41:18.765042Z",
     "shell.execute_reply.started": "2022-10-23T19:41:15.134581Z"
    }
   },
   "outputs": [],
   "source": [
    "#Checking percentage of misspelled words in the data.\n",
    "spell = SpellChecker() \n",
    "# check for misspelled words\n",
    "misspelled = df_train['message'].apply(lambda x: spell.unknown(x))\n",
    "misspelled.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:41:22.694206Z",
     "iopub.status.busy": "2022-10-23T19:41:22.693876Z",
     "iopub.status.idle": "2022-10-23T19:41:47.145704Z",
     "shell.execute_reply": "2022-10-23T19:41:47.144802Z",
     "shell.execute_reply.started": "2022-10-23T19:41:22.694181Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemma(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function modifies the original train dataframe.\n",
    "    A new column for the length of each tweet is added.\n",
    "    The tweets are then tokenized and each word is assigned a part of speech tag \n",
    "    before being lemmatized\n",
    "    \n",
    "    Input:\n",
    "    df: original dataframe\n",
    "        datatype: dataframe \n",
    "        \n",
    "    Output:\n",
    "    df: modified dataframe\n",
    "        datatype: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df['length'] = df['message'].str.len()\n",
    "    df['tokenized'] = df['message'].apply(word_tokenize)\n",
    "    df['pos_tags'] = df['tokenized'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "    \n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    wnl = WordNetLemmatizer()\n",
    "    df['pos_tags'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "    df['lemmatized'] = df['pos_tags'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "    df['lemmatized'] = [' '.join(map(str, l)) for l in df['lemmatized']]  \n",
    "    return df\n",
    "\n",
    "df = lemma(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency\n",
    "Extract and count top most frequently used words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:42:01.570696Z",
     "iopub.status.busy": "2022-10-23T19:42:01.570361Z",
     "iopub.status.idle": "2022-10-23T19:42:02.138312Z",
     "shell.execute_reply": "2022-10-23T19:42:02.137424Z",
     "shell.execute_reply.started": "2022-10-23T19:42:01.570668Z"
    }
   },
   "outputs": [],
   "source": [
    "def frequency(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function determines the frequency of each word in a collection of tweets \n",
    "    and stores the 25 most frequent words in a dataframe, \n",
    "    sorted from most to least frequent\n",
    "    \n",
    "    Input: \n",
    "    tweet: original tweets\n",
    "           datatype: 'str'\n",
    "           \n",
    "    Output: \n",
    "    frequency: dataframe containing the top 25 words \n",
    "               datatype: dataframe          \n",
    "    \"\"\"\n",
    "    \n",
    "    # Count vectorizer excluding english stopwords\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    words = cv.fit_transform(tweet)\n",
    "    \n",
    "    # Count the words in the tweets and determine the frequency of each word\n",
    "    sum_words = words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create a dataframe to store the top 25 words and their frequencies\n",
    "    frequency = pd.DataFrame(words_freq, columns=['word', 'frequency'])\n",
    "    frequency = frequency.head(25)\n",
    "    \n",
    "    return frequency\n",
    "\n",
    "# Extract the top 25 words in each class\n",
    "pro_frequency = frequency(df['lemmatized'][df['sentiment']=='Pro'])\n",
    "anti_frequency = frequency(df['lemmatized'][df['sentiment']=='Anti'])\n",
    "news_frequency = frequency(df['lemmatized'][df['sentiment']=='News'])\n",
    "neutral_frequency = frequency(df['lemmatized'][df['sentiment']=='Neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:42:37.877848Z",
     "iopub.status.busy": "2022-10-23T19:42:37.877519Z",
     "iopub.status.idle": "2022-10-23T19:42:38.212704Z",
     "shell.execute_reply": "2022-10-23T19:42:38.211664Z",
     "shell.execute_reply.started": "2022-10-23T19:42:37.877824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the words in the tweets for the pro and anti climate change classes \n",
    "anti_words = ' '.join([text for text in anti_frequency['word']])\n",
    "pro_words = ' '.join([text for text in pro_frequency['word']])\n",
    "news_words = ' '.join([text for text in news_frequency['word']])\n",
    "neutral_words = ' '.join([text for text in neutral_frequency['word']])\n",
    "\n",
    "# Create wordcloud for the anti climate change class\n",
    "anti_wordcloud = WordCloud(width=800, \n",
    "                           height=500, \n",
    "                           random_state=110, \n",
    "                           max_font_size=110, \n",
    "                           background_color='white',\n",
    "                           colormap=\"Reds\").generate(anti_words)\n",
    "\n",
    "# Create wordcolud for the pro climate change class\n",
    "pro_wordcloud = WordCloud(width=800, \n",
    "                          height=500, \n",
    "                          random_state=73, \n",
    "                          max_font_size=110, \n",
    "                          background_color='white',\n",
    "                          colormap=\"Greens\").generate(pro_words)\n",
    "\n",
    "# Create wordcolud for the news climate change class\n",
    "news_wordcloud = WordCloud(width=800, \n",
    "                          height=500, \n",
    "                          random_state=0, \n",
    "                          max_font_size=110, \n",
    "                          background_color='white',\n",
    "                          colormap=\"Blues\").generate(news_words)\n",
    "\n",
    "# Create wordcolud for the neutral climate change class\n",
    "neutral_wordcloud = WordCloud(width=800, \n",
    "                          height=500, \n",
    "                          random_state=10, \n",
    "                          max_font_size=110, \n",
    "                          background_color='white',\n",
    "                          colormap=\"Oranges\").generate(neutral_words)\n",
    "\n",
    "pro_frequency.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-eight\"></a>\n",
    "# Exploratory data analysis\n",
    "We will explore the structure of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:42:45.308864Z",
     "iopub.status.busy": "2022-10-23T19:42:45.308559Z",
     "iopub.status.idle": "2022-10-23T19:42:45.327891Z",
     "shell.execute_reply": "2022-10-23T19:42:45.326815Z",
     "shell.execute_reply.started": "2022-10-23T19:42:45.308839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for Missing Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:42:48.009763Z",
     "iopub.status.busy": "2022-10-23T19:42:48.009424Z",
     "iopub.status.idle": "2022-10-23T19:42:48.028032Z",
     "shell.execute_reply": "2022-10-23T19:42:48.027126Z",
     "shell.execute_reply.started": "2022-10-23T19:42:48.009736Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "dup_tweets = round((1-(df['message'].nunique()/len(df['message'])))*100,2)\n",
    "print('Percentage of duplicated tweets in train data:')\n",
    "print(dup_tweets,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 10,5% of tweets are duplicated with different ID's. Those duplicate can well be retweets. We will leave the duplicates in our data set for now and re-visit should they have a an impact when fitting the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:42:51.498417Z",
     "iopub.status.busy": "2022-10-23T19:42:51.498084Z",
     "iopub.status.idle": "2022-10-23T19:42:51.514178Z",
     "shell.execute_reply": "2022-10-23T19:42:51.513238Z",
     "shell.execute_reply.started": "2022-10-23T19:42:51.498391Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "print('Number of tweets per sentiment class')\n",
    "df['sentiment'].replace({-1: 'Anti',0:'Neutral',1:'Pro',2:'News'}).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:42:55.063472Z",
     "iopub.status.busy": "2022-10-23T19:42:55.063151Z",
     "iopub.status.idle": "2022-10-23T19:42:55.254876Z",
     "shell.execute_reply": "2022-10-23T19:42:55.253801Z",
     "shell.execute_reply.started": "2022-10-23T19:42:55.063448Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = df[\"sentiment\"].value_counts()\n",
    "plt.bar(range(len(counts)), counts)\n",
    "plt.xticks([0, 1, 2, 3], ['Pro', 'News', 'Neutral', 'Anti'])\n",
    "\n",
    "\n",
    "plt.ylabel(\"Total per class\")\n",
    "plt.xlabel(\"Sentiment Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:42:58.925624Z",
     "iopub.status.busy": "2022-10-23T19:42:58.924583Z",
     "iopub.status.idle": "2022-10-23T19:42:59.136761Z",
     "shell.execute_reply": "2022-10-23T19:42:59.135943Z",
     "shell.execute_reply.started": "2022-10-23T19:42:58.925590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of the length tweets for each class using a box plot\n",
    "sns.boxplot(x=df['sentiment'], y=df['length'], data=df)\n",
    "plt.title('Tweet length for each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:43:02.581166Z",
     "iopub.status.busy": "2022-10-23T19:43:02.580187Z",
     "iopub.status.idle": "2022-10-23T19:43:02.743633Z",
     "shell.execute_reply": "2022-10-23T19:43:02.742907Z",
     "shell.execute_reply.started": "2022-10-23T19:43:02.581133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the proportion of tweets per class\n",
    "plot1 = plt.figure(figsize=(15,5))\n",
    "names = ['Pro','News','Neutral','Anti']\n",
    "perc = df['sentiment'].replace({-1: 'Anti',0:'Neutral',1:'Pro',2:'News'}).value_counts()\n",
    "perc.name = ''\n",
    "perc.plot(kind='pie', labels=names, autopct='%1.1f%%')\n",
    "plt.title('Proportion of tweets in each class',fontsize = 16)\n",
    "plt.figtext(0.12, 0.1, 'figure 1: Percentage of tweets that are classified as either Anti, Pro, Neutral and News',\n",
    "            horizontalalignment='left',fontsize = 14,style='italic')\n",
    "plt.legend(df['sentiment'].replace({-1: 'Anti: Does not believe in man-made climate change',\n",
    "                                          0:'Neutral: Neither believes nor refutes man-made climate change',\n",
    "                                          1:'Pro:Believe in man-made climate change',\n",
    "                                          2:'News: Factual News about climate change'}).value_counts().index,\n",
    "           bbox_to_anchor=(2.3,0.7), loc=\"right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pro climate change is the majority class accounting for about 54% of the data set and 46% being shared amongst the remaining classes. To deal with unbalanced data one can apply resampling to the classes, by adjusting the number of observations in the classes .Below is a  function to deal with the uneven distribution of class labels. The function will modify the number of observations for the classes we need to resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:43:12.780188Z",
     "iopub.status.busy": "2022-10-23T19:43:12.779824Z",
     "iopub.status.idle": "2022-10-23T19:43:12.786256Z",
     "shell.execute_reply": "2022-10-23T19:43:12.785313Z",
     "shell.execute_reply.started": "2022-10-23T19:43:12.780162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create resampling function\n",
    "def resampling(df, class1, class2):\n",
    "    \"\"\" A function takes in a dataframe, a class to be resampled, and a class \n",
    "        thats observations are to be matched with.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df:     Dataframe to be resampled.\n",
    "        class1: Integer of the class that is to be resampled.\n",
    "        class2: Integer of the class whose length is used to resample class1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_resampled:  Resampled dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_class1= df[df.sentiment==class1]\n",
    "    df_class2 = df[df.sentiment==class2]\n",
    "    df_new= df[df.sentiment!=class1]\n",
    "    resampled = resample(df_class1, replace=True, n_samples=len(df_class2.sentiment), random_state=50)\n",
    "    df_resampled = pd.concat([resampled, df_new])    \n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:43:20.975175Z",
     "iopub.status.busy": "2022-10-23T19:43:20.974836Z",
     "iopub.status.idle": "2022-10-23T19:43:20.984929Z",
     "shell.execute_reply": "2022-10-23T19:43:20.984040Z",
     "shell.execute_reply.started": "2022-10-23T19:43:20.975149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a resampled dataset from our clean dataset\n",
    "df_resample = resampling(df_train, -1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:43:18.030110Z",
     "iopub.status.busy": "2022-10-23T19:43:18.029776Z",
     "iopub.status.idle": "2022-10-23T19:43:18.170805Z",
     "shell.execute_reply": "2022-10-23T19:43:18.169931Z",
     "shell.execute_reply.started": "2022-10-23T19:43:18.030084Z"
    }
   },
   "outputs": [],
   "source": [
    "plot2 = plt.figure(figsize=(15,5))\n",
    "names = ['Pro','News','Neutral','Anti']\n",
    "perc = df_resample['sentiment'].replace({-1: 'Anti',0:'Neutral',1:'Pro',2:'News'}).value_counts()\n",
    "perc.name = ''\n",
    "perc.plot(kind='pie', labels=names, autopct='%1.1f%%')\n",
    "plt.title('Proportion of tweets in each class: Resampled dataset',fontsize = 16)\n",
    "plt.figtext(0.12, 0.1, 'figure 2: Percentage of tweets that are classified as either Anti, Pro, Neutral and News (Resampled)',\n",
    "            horizontalalignment='left',fontsize = 14,style='italic')\n",
    "plt.legend(df['sentiment'].replace({-1: 'Anti: Does not believe in man-made climate change',\n",
    "                                          0:'Neutral: Neither believes nor refutes man-made climate change',\n",
    "                                          1:'Pro:Believe in man-made climate change',\n",
    "                                          2:'News: Factual News about climate change'}).value_counts().index,\n",
    "           bbox_to_anchor=(2.3,0.7), loc=\"right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word cloud\n",
    "We will look at the most common words found in the tweets for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:43:25.882787Z",
     "iopub.status.busy": "2022-10-23T19:43:25.882451Z",
     "iopub.status.idle": "2022-10-23T19:43:26.961289Z",
     "shell.execute_reply": "2022-10-23T19:43:26.960232Z",
     "shell.execute_reply.started": "2022-10-23T19:43:25.882758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot pro and anti wordclouds next to one another for comparisson\n",
    "f, axarr = plt.subplots(2,2, figsize=(35,25))\n",
    "axarr[0,0].imshow(pro_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[0,1].imshow(anti_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[1,0].imshow(neutral_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[1,1].imshow(news_wordcloud, interpolation=\"bilinear\")\n",
    "\n",
    "# Remove the ticks on the x and y axes\n",
    "for ax in f.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.axis('off')\n",
    "\n",
    "axarr[0,0].set_title('Words in Pro climate change tweets\\n', fontsize=35)\n",
    "axarr[0,1].set_title('Words in Anti climate change tweets\\n', fontsize=35)\n",
    "axarr[1,0].set_title('Words in Neutral tweets\\n', fontsize=35)\n",
    "axarr[1,1].set_title('Words News tweets\\n', fontsize=35)\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:43:54.845049Z",
     "iopub.status.busy": "2022-10-23T19:43:54.844700Z",
     "iopub.status.idle": "2022-10-23T19:43:55.187943Z",
     "shell.execute_reply": "2022-10-23T19:43:55.186952Z",
     "shell.execute_reply.started": "2022-10-23T19:43:54.845022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the frequent hastags for pro and anti climate change classes\n",
    "sns.barplot(data=pro,y=pro['hashtag'], x=pro['count'])\n",
    "plt.title('Most popular Pro climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:00.075963Z",
     "iopub.status.busy": "2022-10-23T19:44:00.075635Z",
     "iopub.status.idle": "2022-10-23T19:44:00.396839Z",
     "shell.execute_reply": "2022-10-23T19:44:00.395659Z",
     "shell.execute_reply.started": "2022-10-23T19:44:00.075939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the frequent hastags for pro and anti climate change classes\n",
    "sns.barplot(data=anti,y=anti['hashtag'], x=anti['count'])\n",
    "plt.title('Most popular Anti climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:08.180927Z",
     "iopub.status.busy": "2022-10-23T19:44:08.180583Z",
     "iopub.status.idle": "2022-10-23T19:44:08.554172Z",
     "shell.execute_reply": "2022-10-23T19:44:08.553257Z",
     "shell.execute_reply.started": "2022-10-23T19:44:08.180898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the frequent hastags for pro and anti climate change classes\n",
    "sns.barplot(y=neutral['hashtag'], x=neutral['count'])\n",
    "plt.title('Most popular Neutral climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:15.326353Z",
     "iopub.status.busy": "2022-10-23T19:44:15.326028Z",
     "iopub.status.idle": "2022-10-23T19:44:15.335944Z",
     "shell.execute_reply": "2022-10-23T19:44:15.335289Z",
     "shell.execute_reply.started": "2022-10-23T19:44:15.326326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train & validation for model training\n",
    "\n",
    "# Seperate features and tagret variables\n",
    "X = df_resample['message']\n",
    "y = df_resample['sentiment']\n",
    "\n",
    "# Split the train data to create validation dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:16.646695Z",
     "iopub.status.busy": "2022-10-23T19:44:16.646377Z",
     "iopub.status.idle": "2022-10-23T19:44:16.653693Z",
     "shell.execute_reply": "2022-10-23T19:44:16.652847Z",
     "shell.execute_reply.started": "2022-10-23T19:44:16.646669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "               ('clf', RandomForestClassifier(max_depth=5, \n",
    "                                              n_estimators=100))])\n",
    "\n",
    "# Naïve Bayes:\n",
    "nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "               ('clf', MultinomialNB())])\n",
    "\n",
    "# K-NN Classifier\n",
    "knn = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('clf', KNeighborsClassifier(n_neighbors=5, \n",
    "                                             metric='minkowski', \n",
    "                                             p=2))])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = Pipeline([('tfidf',TfidfVectorizer()),\n",
    "               ('clf',LogisticRegression(C=1, \n",
    "                                         class_weight='balanced', \n",
    "                                         max_iter=1000))])\n",
    "# Linear SVC:\n",
    "lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                 ('clf', LinearSVC(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:20.324539Z",
     "iopub.status.busy": "2022-10-23T19:44:20.324226Z",
     "iopub.status.idle": "2022-10-23T19:44:28.537619Z",
     "shell.execute_reply": "2022-10-23T19:44:28.536522Z",
     "shell.execute_reply.started": "2022-10-23T19:44:20.324515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random forest \n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_valid)\n",
    "\n",
    "# Niave bayes\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_valid)\n",
    "\n",
    "# K - nearest neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_valid)\n",
    "\n",
    "# Linear regression\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_valid)\n",
    "\n",
    "# Linear SVC\n",
    "lsvc.fit(X_train, y_train)\n",
    "y_pred_lsvc = lsvc.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:32.162792Z",
     "iopub.status.busy": "2022-10-23T19:44:32.162455Z",
     "iopub.status.idle": "2022-10-23T19:44:32.410177Z",
     "shell.execute_reply": "2022-10-23T19:44:32.409159Z",
     "shell.execute_reply.started": "2022-10-23T19:44:32.162766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a classification Report for the random forest model\n",
    "print(metrics.classification_report(y_valid, y_pred_rf))\n",
    "\n",
    "# Generate a normalized confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_rf)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"plasma\", \n",
    "            xticklabels=rf.classes_, \n",
    "            yticklabels=rf.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('Random forest classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:45.370831Z",
     "iopub.status.busy": "2022-10-23T19:44:45.370499Z",
     "iopub.status.idle": "2022-10-23T19:44:45.626849Z",
     "shell.execute_reply": "2022-10-23T19:44:45.625750Z",
     "shell.execute_reply.started": "2022-10-23T19:44:45.370804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a classification Report for the Naive Bayes model\n",
    "print(metrics.classification_report(y_valid, y_pred_nb))\n",
    "\n",
    "# Generate a normalized confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_nb)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"plasma\", \n",
    "            xticklabels=nb.classes_, \n",
    "            yticklabels=nb.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('Naive Bayes classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:50.528838Z",
     "iopub.status.busy": "2022-10-23T19:44:50.528518Z",
     "iopub.status.idle": "2022-10-23T19:44:50.749166Z",
     "shell.execute_reply": "2022-10-23T19:44:50.748156Z",
     "shell.execute_reply.started": "2022-10-23T19:44:50.528814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a classification Report for the K-nearest neighbors model\n",
    "print(metrics.classification_report(y_valid, y_pred_knn))\n",
    "\n",
    "# Generate a normalized confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_knn)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"plasma\", \n",
    "            xticklabels=knn.classes_, \n",
    "            yticklabels=knn.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('K - nearest neighbors classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:44:55.884304Z",
     "iopub.status.busy": "2022-10-23T19:44:55.883540Z",
     "iopub.status.idle": "2022-10-23T19:44:56.113332Z",
     "shell.execute_reply": "2022-10-23T19:44:56.112450Z",
     "shell.execute_reply.started": "2022-10-23T19:44:55.884276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a classification Report for the model\n",
    "print(metrics.classification_report(y_valid, y_pred_lr))\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred_lr)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"plasma\", \n",
    "            xticklabels=lr.classes_, \n",
    "            yticklabels=lr.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('Logistic regression classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:45:00.797805Z",
     "iopub.status.busy": "2022-10-23T19:45:00.796983Z",
     "iopub.status.idle": "2022-10-23T19:45:01.037310Z",
     "shell.execute_reply": "2022-10-23T19:45:01.036426Z",
     "shell.execute_reply.started": "2022-10-23T19:45:00.797767Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a classification Report for the linear SVC model\n",
    "print(metrics.classification_report(y_valid, y_pred_lsvc))\n",
    "\n",
    "# Generate a normalized confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_lsvc)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"plasma\", \n",
    "            xticklabels=lsvc.classes_, \n",
    "            yticklabels=lsvc.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('Linear SVC classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "The Linear SVC Classification model achieved the highest F1 score of 0.80. This will be our model of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Comet experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving each metric to add to a dictionary for logging\n",
    "f1 = f1_score(y_valid, y_pred, average='weighted')\n",
    "precision = precision_score(y_valid, y_pred, average='weighted')\n",
    "recall = recall_score(y_valid, y_pred, average='weighted')\n",
    "\n",
    "# Create dictionaries for the data we want to log          \n",
    "metrics = {\"f1\": f1,\n",
    "           \"recall\": recall,\n",
    "           \"precision\": precision}\n",
    "\n",
    "params= {'classifier': 'linear SVC',\n",
    "         'max_df': 0.8,\n",
    "         'min_df': 2,\n",
    "         'ngram_range': '(1,2)',\n",
    "         'vectorizer': 'Tfidf',\n",
    "         'scaling': 'no',\n",
    "         'resampling': 'no',\n",
    "         'test_train random state': '0'}\n",
    "  \n",
    "# Log info on comet\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.log_parameters(params)\n",
    "\n",
    "# End experiment\n",
    "experiment.end()\n",
    "\n",
    "# Display results on comet page\n",
    "experiment.display()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-23T19:45:15.522924Z",
     "iopub.status.busy": "2022-10-23T19:45:15.522580Z",
     "iopub.status.idle": "2022-10-23T19:45:15.561675Z",
     "shell.execute_reply": "2022-10-23T19:45:15.560730Z",
     "shell.execute_reply.started": "2022-10-23T19:45:15.522895Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/edsa-sentiment-classification/test.csv')\n",
    "y_test = lsvc.predict(test['message'])\n",
    "output = pd.DataFrame({'tweetid': test.tweetid,\n",
    "                       'sentiment': y_test})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
